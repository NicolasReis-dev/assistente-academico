# Importa os m√≥dulos necess√°rios do Flask e outras bibliotecas
import os
from dotenv import load_dotenv
from flask import Flask, Response, render_template, request, stream_with_context
load_dotenv()
import google.generativeai as genai  # biblioteca da API do Gemini
import traceback  # para exibir erros detalhados no terminal

# Cria a aplica√ß√£o Flask
app = Flask(__name__)

# =======================
# CONFIGURA√á√ÉO DO GEMINI
# =======================

# Chave de API para acessar o modelo do Google Gemini
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Configura a biblioteca com a chave de API
genai.configure(api_key=GEMINI_API_KEY)

# Define qual modelo do Gemini ser√° usado
model = genai.GenerativeModel('gemini-2.5-flash-preview-05-20')
print("‚úì Modelo configurado: gemini-2.5-flash-preview-05-20")

# =======================
# CARREGAR BASE DE DADOS
# =======================


def carregar_base_dados():
    """Tenta abrir o arquivo dados_academicos.txt com fallback de encoding."""
    for encoding in ("utf-8", "latin-1", "utf-16"):
        try:
            with open("dados_academicos.txt", "r", encoding=encoding) as f:
                texto_completo = f.read()
            print(f"‚úì Base carregada com encoding: {encoding}")
            return texto_completo
        except UnicodeDecodeError:
            continue
        except FileNotFoundError:
            print("‚ö† Arquivo 'dados_academicos.txt' n√£o encontrado.")
            return None
    print("‚ùå Nenhum encoding compat√≠vel encontrado para 'dados_academicos.txt'.")
    return None


try:
    texto_completo = carregar_base_dados()
    if texto_completo:
        # Divide o texto em partes separadas por duas quebras de linha
        dados_base = [p.strip()
                      for p in texto_completo.split("\n\n") if p.strip()]

        # Calcula o tamanho total em caracteres
        total = sum(len(p) for p in dados_base)

        # Se o texto for muito grande, corta parte para n√£o estourar limite da API
        if total > 30000:
            print(f"‚ö† Base muito grande ({total} chars), reduzindo...")
            nova = []
            atual = 0
            for p in dados_base:
                if atual + len(p) < 30000:
                    nova.append(p)
                    atual += len(p)
                else:
                    break
            dados_base = nova
            print(f"‚úì Base reduzida ({atual} chars)")
        else:
            print(f"‚úì Base carregada ({total} chars)")
    else:
        dados_base = ["Nenhum dado dispon√≠vel no momento."]

except Exception as e:
    print("‚ùå Erro ao carregar base de dados:", traceback.format_exc())
    dados_base = ["Nenhum dado dispon√≠vel no momento."]

# =======================
# CONFIGURA√á√ÉO DO PROMPT
# =======================

sistema_prompt = (
    "Voc√™ √© um assistente acad√™mico da UniEVANG√âLICA. "
    "Seja DIRETO e OBJETIVO nas respostas. "
    "Responda em no m√°ximo 4-5 linhas, usando par√°grafos simples SEM asteriscos, SEM listas e SEM bullet points. "
    "Use as informa√ß√µes fornecidas para responder. "
    "Se n√£o souber a resposta com base nesses dados, diga apenas "
    "'Poxa, n√£o tenho essa informa√ß√£o dispon√≠vel!! Mas posso continuar te ajudando com outros assuntos, como seu calend√°rio de aulas ou notas das avalia√ß√µes'.\n\n"
    f"--- BASE DE DADOS ---\n{' '.join(dados_base)}\n--- FIM DA BASE ---"
)

# Lista que armazena as perguntas e respostas anteriores
historico_chat = []

# =======================
# FUN√á√ÉO PRINCIPAL DE RESPOSTA
# =======================


def responder_avancado(pergunta):
    """Gera a resposta do chatbot em modo streaming."""
    try:
        print(f"\nüì® Pergunta: {pergunta[:100]}")
        mensagem = f"{sistema_prompt}\n\nPergunta do usu√°rio: {pergunta}"

        if len(mensagem) > 100000:
            yield "Desculpe, a base est√° muito grande. Contate o administrador."
            return

        # Gera a resposta em streaming
        response = model.generate_content(mensagem, stream=True)

        # Itera sobre os chunks da resposta e envia para o front-end
        for chunk in response:
            if chunk.text:
                yield chunk.text

    except Exception:
        print("‚ùå ERRO:", traceback.format_exc())
        yield "Erro ao processar a pergunta."


# =======================
# ROTAS DO SITE
# =======================

@app.route('/')
def login():
    return render_template('login.html')


@app.route('/home')
def home():
    return render_template('home.html')


@app.route('/assistente')
def assistente():
    return render_template('assistente.html')


@app.route('/stream')
def stream():
    pergunta = request.args.get('mensagem', '').strip()
    if not pergunta:
        return Response("Nenhuma pergunta fornecida.", status=400)

    # Usa um gerador para enviar a resposta em partes (streaming)
    def generate():
        for chunk in responder_avancado(pergunta):
            yield f"data: {chunk}\n\n"

    # Retorna a resposta como um evento de stream
    return Response(stream_with_context(generate()), mimetype='text/event-stream')


# =======================
# EXECU√á√ÉO
# =======================
if __name__ == '__main__':
    app.run(debug=True)
