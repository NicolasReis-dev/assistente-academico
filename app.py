# Importa os m√≥dulos necess√°rios do Flask e outras bibliotecas
import os
from dotenv import load_dotenv
from flask import Flask, render_template, request, redirect, url_for
load_dotenv()
import google.generativeai as genai  # biblioteca da API do Gemini
import traceback  # para exibir erros detalhados no terminal

# Cria a aplica√ß√£o Flask
app = Flask(__name__)

# =======================
# CONFIGURA√á√ÉO DO GEMINI
# =======================

# Chave de API para acessar o modelo do Google Gemini
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Configura a biblioteca com a chave de API
genai.configure(api_key=GEMINI_API_KEY)

# Define qual modelo do Gemini ser√° usado
model = genai.GenerativeModel('gemini-2.5-flash-preview-05-20')
print("‚úì Modelo configurado: gemini-2.5-flash-preview-05-20")

# =======================
# CARREGAR BASE DE DADOS
# =======================


def carregar_base_dados():
    """Tenta abrir o arquivo dados_academicos.txt com fallback de encoding."""
    for encoding in ("utf-8", "latin-1", "utf-16"):
        try:
            with open("dados_academicos.txt", "r", encoding=encoding) as f:
                texto_completo = f.read()
            print(f"‚úì Base carregada com encoding: {encoding}")
            return texto_completo
        except UnicodeDecodeError:
            continue
        except FileNotFoundError:
            print("‚ö† Arquivo 'dados_academicos.txt' n√£o encontrado.")
            return None
    print("‚ùå Nenhum encoding compat√≠vel encontrado para 'dados_academicos.txt'.")
    return None


try:
    texto_completo = carregar_base_dados()
    if texto_completo:
        # Divide o texto em partes separadas por duas quebras de linha
        dados_base = [p.strip()
                      for p in texto_completo.split("\n\n") if p.strip()]

        # Calcula o tamanho total em caracteres
        total = sum(len(p) for p in dados_base)

        # Se o texto for muito grande, corta parte para n√£o estourar limite da API
        if total > 30000:
            print(f"‚ö† Base muito grande ({total} chars), reduzindo...")
            nova = []
            atual = 0
            for p in dados_base:
                if atual + len(p) < 30000:
                    nova.append(p)
                    atual += len(p)
                else:
                    break
            dados_base = nova
            print(f"‚úì Base reduzida ({atual} chars)")
        else:
            print(f"‚úì Base carregada ({total} chars)")
    else:
        dados_base = ["Nenhum dado dispon√≠vel no momento."]

except Exception as e:
    print("‚ùå Erro ao carregar base de dados:", traceback.format_exc())
    dados_base = ["Nenhum dado dispon√≠vel no momento."]

# =======================
# CONFIGURA√á√ÉO DO PROMPT
# =======================

sistema_prompt = (
    "Voc√™ √© um assistente acad√™mico da UniEVANG√âLICA. "
    "Seja DIRETO e OBJETIVO nas respostas. "
    "Responda em no m√°ximo 4-5 linhas, usando par√°grafos simples SEM asteriscos, SEM listas e SEM bullet points. "
    "Use as informa√ß√µes fornecidas para responder. "
    "Se n√£o souber a resposta com base nesses dados, diga apenas "
    "'Poxa, n√£o tenho essa informa√ß√£o dispon√≠vel!! Mas posso continuar te ajudando com outros assuntos, como seu calend√°rio de aulas ou notas das avalia√ß√µes'.\n\n"
    f"--- BASE DE DADOS ---\n{' '.join(dados_base)}\n--- FIM DA BASE ---"
)

# Lista que armazena as perguntas e respostas anteriores
historico_chat = []

# =======================
# FUN√á√ÉO PRINCIPAL DE RESPOSTA
# =======================


def responder_avancado(pergunta):
    try:
        # Mostra a pergunta no terminal (debug)
        print(f"\nüì® Pergunta: {pergunta[:100]}")

        # Junta o prompt do sistema com a pergunta feita pelo usu√°rio
        mensagem = f"{sistema_prompt}\n\nPergunta do usu√°rio: {pergunta}"

        # Verifica se a mensagem n√£o est√° muito grande (limite da API)
        if len(mensagem) > 100000:
            return "Desculpe, a base est√° muito grande. Contate o administrador."

        # Envia a mensagem para o modelo e recebe a resposta
        response = model.generate_content(mensagem)
        resposta = response.text.strip()

        # Armazena no hist√≥rico (opcional)
        historico_chat.append({"usuario": pergunta, "assistente": resposta})

        # Retorna o texto de resposta para o front-end
        return resposta

    except Exception:
        print("‚ùå ERRO:", traceback.format_exc())
        return "Erro ao processar a pergunta."


# =======================
# ROTAS DO SITE
# =======================

@app.route('/')
def login():
    return render_template('login.html')


@app.route('/home')
def home():
    return render_template('home.html')


@app.route('/assistente', methods=['GET', 'POST'])
def assistente():
    resposta = ""
    pergunta = ""
    if request.method == 'POST':
        pergunta = request.form.get('mensagem', '').strip()
        if pergunta:
            resposta = responder_avancado(pergunta)
    return render_template('assistente.html', resposta=resposta, pergunta=pergunta)


# =======================
# EXECU√á√ÉO
# =======================
if __name__ == '__main__':
    app.run(debug=True)
